# ‚úÖ COMPLETE SETUP CHECKLIST - REAL SCRAPING + DATABASE

**Follow this EXACTLY - No skipping steps!**

---

## üìã CHECKLIST: REAL SYSTEM SETUP

### ‚úÖ PART 1: SUPABASE DATABASE (Do First!)
- [ ] Go to https://supabase.com
- [ ] Sign up with student email (free, no credit card!)
- [ ] Create project named "ethical_shopping"
- [ ] Wait 1-2 minutes for database creation
- [ ] Go to Settings ‚Üí Database
- [ ] Copy PostgreSQL connection string
- [ ] **SAVE IT SOMEWHERE** (you'll need it!)

**Looks like:** `postgresql://postgres:password@aws-0-xxxxx.pooler.supabase.com:6543/postgres`

---

### ‚úÖ PART 2: UPDATE LOCAL .env.local FILE
- [ ] Open: `backend/.env.local`
- [ ] Find line: `DATABASE_URL=postgresql://...`
- [ ] Replace with your Supabase connection string from PART 1
- [ ] **Example:**
```
DATABASE_URL=postgresql://postgres:MyPassword123!@aws-0-us-east-1.pooler.supabase.com:6543/postgres
```
- [ ] Save file
- [ ] Run: `cat backend\.env.local` to verify

---

### ‚úÖ PART 3: CREATE DATABASE TABLES
- [ ] Terminal: Open in project root directory
- [ ] Run:
```powershell
cd backend
npx prisma migrate dev --name init
```
- [ ] Wait for migration to complete
- [ ] You should see: ‚úÖ "5 tables migrated"

---

### ‚úÖ PART 4: VERIFY DATABASE TABLES
- [ ] Go to Supabase dashboard (in browser)
- [ ] Click "SQL Editor"
- [ ] Run this query:
```sql
SELECT tablename FROM pg_tables WHERE schemaname='public';
```
- [ ] You should see these tables:
  - `Product`
  - `Analysis`
  - `Alternative`
  - `PriceHistory`
  - `RuleEvaluation`

**If you see all 5 tables: ‚úÖ DATABASE READY!**

---

### ‚úÖ PART 5: RESTART BACKEND
- [ ] Stop backend: Press **Ctrl+C** in backend terminal
- [ ] Restart backend:
```powershell
npm run start:dev --workspace=backend
```
- [ ] Wait for: `üöÄ Backend server running on http://localhost:4000`
- [ ] You should see:
  - ‚úÖ "Google Gemini AI initialized successfully"
  - ‚úÖ "Nest application successfully started"

---

### ‚úÖ PART 6: TEST REAL FLOW
- [ ] Open: http://localhost:3000 (frontend)
- [ ] Paste a product URL:
  ```
  https://www.amazon.in/dp/B0B85JQHBX
  ```
  Or any real Amazon/product link
- [ ] Click "Analyze"
- [ ] Wait 3-5 seconds for analysis
- [ ] You should see:
  - Product title
  - Ethical score
  - Deal score
  - Recommendations
  - **REAL DATA (not mocked!)**

---

### ‚úÖ PART 7: VERIFY DATA IN DATABASE
- [ ] Go to Supabase dashboard
- [ ] Click "SQL Editor"
- [ ] Run:
```sql
SELECT "title", "price", "ethicalScore", "dealScore", "createdAt" 
FROM "Analysis" 
ORDER BY "createdAt" DESC 
LIMIT 5;
```
- [ ] You should see your analysis data saved!
- [ ] **If you see data: ‚úÖ EVERYTHING WORKING!**

---

### ‚úÖ PART 8: FINAL VERIFICATION
- [ ] Frontend working: http://localhost:3000 ‚úÖ
- [ ] Backend running: http://localhost:4000 ‚úÖ
- [ ] Database connected: Supabase ‚úÖ
- [ ] Data persisting: Checked in SQL ‚úÖ
- [ ] Real scraping: Getting real product data ‚úÖ
- [ ] Real AI: Gemini analysis ‚úÖ

---

## üéâ IF ALL CHECKMARKS ARE DONE

**YOU NOW HAVE A REAL SYSTEM!**

Not mock data. Not fake analysis. **REAL WORKING APPLICATION** ‚úÖ

- ‚úÖ Scraping real products from websites
- ‚úÖ AI analyzing with Google Gemini
- ‚úÖ Saving all data to database
- ‚úÖ Free tier suitable for student projects
- ‚úÖ Ready to scale

---

## ‚ö†Ô∏è IF SOMETHING GOES WRONG

### Problem: "Connection refused"
**Solution:** Check your DATABASE_URL in `.env.local` - copy it exactly from Supabase

### Problem: "Can't find tables"
**Solution:** Run `npx prisma migrate dev --name init` again

### Problem: "Frontend not connecting to backend"
**Solution:** Make sure backend is running on port 4000

### Problem: "No data in database after analysis"
**Solution:** 
1. Check backend logs for errors
2. Verify internet connection
3. Try different product URL

### Problem: "Scraping failed"
**Solution:** That's normal! Some sites block bots. Backend has fallback to use mock data temporarily.

---

## üìû NEXT STEPS (When This Works)

1. ‚úÖ Test with 5-10 different products
2. ‚úÖ Check database has all analyses
3. ‚úÖ Verify data quality
4. ‚úÖ Then deploy to Vercel
5. ‚úÖ Add price tracking (daily cron)
6. ‚úÖ Build more features

---

## üöÄ DEPLOYMENT READY?

Once you verify everything works locally:

```powershell
git add .
git commit -m "Local database testing complete - ready for production"
git push
```

This will trigger Vercel to deploy with:
- ‚úÖ Real Gemini AI
- ‚úÖ Real database (you set up Supabase)
- ‚úÖ Real scraping
- ‚úÖ **LIVE PRODUCTION SYSTEM**

---

## ‚ú® WHAT YOU'VE BUILT

- **Frontend:** Next.js with real-time analysis
- **Backend:** NestJS with Gemini AI
- **Database:** PostgreSQL (Supabase)
- **Scraping:** Real HTML parsing
- **Cost:** $0/month (all free tiers)
- **Status:** Production ready

**This is a REAL project for your portfolio!** üéä

---

**Start with PART 1 above. Follow each step exactly. You've got this!** üí™
